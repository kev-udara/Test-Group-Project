# Global UFO Sightings Explorer

An end-to-end platform to ingest, enrich, and visualize global UFO reports alongside meteor showers, air traffic, and light pollution.

---

## ğŸ“ Project Structure

```text
.
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ dashboard.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ ingest/
â”‚   â”œâ”€â”€ ufo_ingest.py
â”‚   â”œâ”€â”€ meteor_enrich.py
â”‚   â”œâ”€â”€ light_pollution_enrich.py
â”‚   â”œâ”€â”€ flight_enrich.py
â”‚   â”œâ”€â”€ check_enrich.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ nuforc.json
â”‚   â”œâ”€â”€ ufo_sightings.csv
â”‚   â”œâ”€â”€ air_traffic_data.csv
â”‚   â””â”€â”€ meteor_showers.csv
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md   â† you are here
```
---

## âš™ï¸ Prerequisites

- **Docker** (Engine â‰¥ 19.03)  
- **Docker Compose** (v1.27+)  

---

## ğŸ”‘ Environment Variables

Create a `.env` file in the project root with your API keys:

```dotenv
# Elasticsearch (optional override)
ES_HOST=http://elasticsearch:9200

# Kaggle (for UFO CSV)
KAGGLE_USERNAME=<your_username>
KAGGLE_KEY=<your_key>

# Optional enrichment APIs
NOAA_TOKEN=<your_noaa_token>
OPENSKY_USER=<your_opensky_user>
OPENSKY_PASS=<your_opensky_pass>
WEATHERBIT_KEY=<your_weatherbit_key>
```
---

## ğŸš€ Running with Docker Compose

**1. Start Elasticsearch & Kibana**
```bash
docker-compose up -d elasticsearch kibana
```
Wait until the Elasticsearch healthcheck passes.

**2. Build & run the Ingest service**
```bash
docker-compose build ingest
docker-compose up ingest
```
This will:
	â€¢	Merge NUFORC + Kaggle UFO data
	â€¢	Parse and clean timestamps and locations
	â€¢	Enrich with meteor showers, light pollution, and air traffic
	â€¢	Index into the Elasticsearch ufo_sightings index

 **3. Build & run the Dashboard service**
 ```bash
docker-compose build dashboard
docker-compose up dashboard
```
â€¢	Visit http://localhost:8000 to explore the Streamlit dashboard.

 **4. Tear down**
 ```bash
docker-compose down
```
